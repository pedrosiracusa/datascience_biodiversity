{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Co-working networks for assisted names remapping</center>\n",
    "## <center>Species occurrence dataset cleaning</center>\n",
    "#### <center>Author: Pedro C. de Siracusa</center>\n",
    "#### <center>Date: Oct 9, 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As collectors co-working network models are built using collectors ids, their quality is strictly related to the quality of fields holding authorship information, such as `recordedBy`. For records that are authored by more than one collectors the recommended practice is to include all names in a single string, separating them using the same delimiter across all records. However, as collector authorship information is traditionally not very relevant for biodiversity data users, this field is usually overlooked by dataset curators. It is a common practice only to include the nominal collector (or first collector) in the collection authorship field, while including secondary collectors as 'et al.' or even omitting them. Such information loss might obfuscate important relationships that should have been included in the network models. In this notebook I'll demonstrate how we could get some assistance for remapping names by using co-working networks. By comparing candidate identities (names that are sufficiently similar) in terms of their neighborhoods connectivity, I will try to identify the most important heteronymous entities in the dataset. This information will then be stored in a **Names Map** for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three important measures of quality for the collection authorship field are \n",
    "1. Consistent use of delimiters across all records, which should not be used elsewhere if not for separating names of distinct entities;\n",
    "2. Consistent entities naming standard, for example using last name separated from initials with a comma. This would avoid dealing with heteronymous entities, those which have non-unique identifiers;\n",
    "3. Information completeness, meaning that all collectors responsible for a record should ideally be included in the authorship field;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although homonymous entities (different entities holding the same name) are also possible issues for the network model, these cases are harder to detect than heteronymous entities. One possible way is to detect anomalies in the entity's activity records, as demonstrated for `'kuhlmann,M'` in notebook 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import modules.cleaning.names as nc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsetPath = '/home/pedro/datasets/ub_herbarium/occurrence.txt'\n",
    "\n",
    "cols = [ 'recordedBy','scientificName','taxonRank', 'collectionCode','eventDate']\n",
    "occs = pd.read_csv(dsetPath, sep='\\t', usecols=cols)\n",
    "occs=occs[occs['recordedBy'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collectionCode</th>\n",
       "      <th>recordedBy</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>taxonRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UB</td>\n",
       "      <td>Irwin, HS</td>\n",
       "      <td>1972-01-16T01:00Z</td>\n",
       "      <td>Annona monticola Mart.</td>\n",
       "      <td>SPECIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UB</td>\n",
       "      <td>Ratter, JA; et al.</td>\n",
       "      <td>1976-06-30T01:00Z</td>\n",
       "      <td>Myracrodruon urundeuva Allem.</td>\n",
       "      <td>SPECIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UB</td>\n",
       "      <td>Heringer, EP</td>\n",
       "      <td>1954-06-05T01:00Z</td>\n",
       "      <td>Myracrodruon urundeuva Allem.</td>\n",
       "      <td>SPECIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UB</td>\n",
       "      <td>Coelho, JP</td>\n",
       "      <td>1964-10-15T01:00Z</td>\n",
       "      <td>Myracrodruon urundeuva Allem.</td>\n",
       "      <td>SPECIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UB</td>\n",
       "      <td>Eiten, G; Eiten, LT</td>\n",
       "      <td>1963-08-17T01:00Z</td>\n",
       "      <td>Myracrodruon urundeuva Allem.</td>\n",
       "      <td>SPECIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  collectionCode           recordedBy          eventDate  \\\n",
       "0             UB            Irwin, HS  1972-01-16T01:00Z   \n",
       "1             UB   Ratter, JA; et al.  1976-06-30T01:00Z   \n",
       "2             UB         Heringer, EP  1954-06-05T01:00Z   \n",
       "3             UB           Coelho, JP  1964-10-15T01:00Z   \n",
       "4             UB  Eiten, G; Eiten, LT  1963-08-17T01:00Z   \n",
       "\n",
       "                  scientificName taxonRank  \n",
       "0         Annona monticola Mart.   SPECIES  \n",
       "1  Myracrodruon urundeuva Allem.   SPECIES  \n",
       "2  Myracrodruon urundeuva Allem.   SPECIES  \n",
       "3  Myracrodruon urundeuva Allem.   SPECIES  \n",
       "4  Myracrodruon urundeuva Allem.   SPECIES  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Atomizing names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the names atomization step our task is to retrieve names referring to individual entities from names strings. It must be noted that even in datasets for which the collection authorship field has high quality some records may still be inconsistently formatted. In order to detect such inconsistencies one possible approach is to first apply the 'standard' atomization function to every record, such that most records are initially atomized. Next we manually inspect very short and very long names strings and decide which of them should have been split. Finally we must map different atomization functions to particular cases or even correct some of them manually. The functions in the cell below were refactored from previous notebooks for this approach. The `atomizeNames` function is given a default atomization operation, but also takes an extra argument `replaces` holding names to be reatomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getNamesList( col, with_counts=False, orderBy=None ):\n",
    "    \"\"\"\n",
    "    Gets a list of names from an atomized names column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    with_counts : bool\n",
    "        If set to True the result includes the number of records by each collector.\n",
    "    \n",
    "    orderBy : str\n",
    "        If some rule is specified, the resulting list is sorted. Rules can be either\n",
    "        to sort alphabetically ('alphabetic') or by the number of records by each\n",
    "        collector ('counts').   \n",
    "    \"\"\"\n",
    "    \n",
    "    if orderBy not in [ None, \"alphabetic\", \"counts\"]:\n",
    "        raise ValueError(\"Invalid argument for 'orderBy': {}\".format(orderBy))\n",
    "    \n",
    "    if with_counts or orderBy==\"counts\":\n",
    "        l = [ (n,c) for (n,c) in Counter( n for nlst in col for n in nlst ).items() ] \n",
    "        if orderBy==\"alphabetic\":\n",
    "            return sorted( l, key=lambda x: x[0] )\n",
    "        elif orderBy==\"counts\":\n",
    "            sorted_l = sorted( l, key=lambda x: x[1], reverse=True )\n",
    "            if with_counts:\n",
    "                return sorted_l\n",
    "            else:\n",
    "                return [ n for (n,c) in sorted_l ]\n",
    "        else:\n",
    "            return l\n",
    "                \n",
    "    else:\n",
    "        if orderBy==\"alphabetic\":\n",
    "            return sorted(list(set( n for nlst in col for n in nlst )))\n",
    "        else:\n",
    "            return list(set( n for nlst in col for n in nlst ))\n",
    "        \n",
    "        \n",
    "def getRecordsBy(df, colName, name):\n",
    "    return df.loc[df[colName].apply( lambda x: name in x )]\n",
    "\n",
    "\n",
    "def atomizeNames( col, operation=None, replaces=None ):\n",
    "    \"\"\"\n",
    "    Applies an atomization operation on a names column, which must be a pandas Series. \n",
    "    The atomized names at each row are stored as a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : pandas.Series\n",
    "        Names column to be atomized.\n",
    "        \n",
    "    operation : function\n",
    "        The atomization operation to be applied to the names column\n",
    "        \n",
    "    replaces:\n",
    "        A list of 2-tuples (srclst, tgt), where srclst is a list of names to be replaced by tgt.\n",
    "        The element tgt can be either a string or a function which results in a string.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        A pandas Series with lists of atomized names.\n",
    "    \"\"\"\n",
    "    if replaces is not None:\n",
    "        replacesDict = dict( (src, tgt(src)) if callable(tgt) else (src,tgt) for (srclst, tgt) in replaces for src in srclst )\n",
    "        col = col.replace( replacesDict )\n",
    "        \n",
    "    col_atomized = col.apply( operation )\n",
    "    return col_atomized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define a default atomizing function, which retrieves names from strings assuming they're separated by a `';'` delimiter. Then I add a new column `recordedBy_atomized` to my original data frame, containing atomized collectors names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atomizingFunction = lambda x: nc.namesFromString(x,delim=';') \n",
    "atomized_recordedBy = atomizeNames(occs['recordedBy'],operation=atomizingFunction)\n",
    "occs['recordedBy_atomized'] = atomized_recordedBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some candidates to be reatomized. For short names it is easier to inspect if we get the original names string associated with each atomized name. First I'll inspect the 100 shortest names and then the 100 longest names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest names\n",
      "name\tin\tnamestring\n",
      "==========================\n",
      "O\tin\tOliveira, RC; Moura, CO; Cardoso, AGT; Sonsin, J; Cordeiro, AOO; Million, JL; Antunes, LLC; O\n",
      "S\tin\tMartins, DS; Câmara, PEAS; Amorim, PRF; Costa, DP; Faria, JEQ; Carvalho, AM; Gonzaga, RMO; S\n",
      "?\tin\t?\n",
      "R\tin\tFarias, R; Carvalho, AM; Carvalho, JA; Fonsêca, LM; Proença, CEB; Potzernheim, ML; R\n",
      "F\tin\tF\n",
      ".\tin\tFaria, JEQ; Carvalho-Silva, M; Câmara, PEAS; .; Soares, AER; Teixeira Júnior, AQ; Benedete\n",
      "P\tin\tSasaki, D; Pedroga, JA; Corrêa, TR; P; Piva, JH\n",
      "Si\tin\tRatter, JA; Bridgewater, S; Cardoso, E; Lima, V; Munhoz, CBR; Oliveira, NR; Ribeiro, JF; Si\n",
      "RR\tin\tIrwin, HS; Souza, R; Santos; RR\n",
      "FO\tin\tFO\n",
      "E.\tin\tHällström; E.\n",
      "M.\tin\tHatschbach, G; M.\n",
      "K.\tin\tYushun.; K.\n",
      "Nu\tin\tFaria, JEQ; Campos, LZO; Ibrahim, M; Martins, RC; Caires, CS; Meneguzzo, TEC; Souza, LF; Nu\n",
      "Fl\tin\tLucas, EJ; Mazine-Capelo, FF; Kollmann, L; Brummitt, NA; Campos, OR; Fl\n",
      "Cid\tin\tCid; Ramos; Mota; Rosas\n",
      "DAF\tin\tDAF\n",
      "Bid\tin\tGuedes, ML; Bid; Carla\n",
      "Ono\tin\tKirkbride Junior, JH; Ono; E.K.M; et al.\n",
      "PKL\tin\tPKL; Eliana\n",
      "PLK\tin\tMárcio; PLK\n",
      "Ben\tin\tFaria, JEQ; Carvalho-Silva, M; Câmara, PEAS; Gama, R; Soares, AER; Teixeira Júnior, AQ; Ben\n",
      "Car\tin\tFaria, JEQ; Câmara, PEAS; Costa, DP; Martins, DS; Amorim, PRF; Sousa, RV; Gonzaga, RMO; Car\n",
      "Ros\tin\tPirani, JR; Furlan, A; Cordeiro, I; Amaral, MCE; Menezes, NL; Ros\n",
      "Ana\tin\tBarroso, GM; Ana; Maria, J\n",
      "Bw.\tin\tBw.\n",
      "Iri\tin\tIri\n",
      "Edy\tin\tBenício; Nelson; Edy\n",
      "Ule\tin\tUle\n",
      "JJB\tin\tJJB\n",
      "Igor\tin\tTiago; Ana Clara; Igor; Tayas, C\n",
      "P.G.\tin\tCarboni, M; Faraco, AG; Soares; P.G.; Sampaio, D; Breier, TB\n",
      "C.M.\tin\tLammers, TG; Rodriguez, R; Baeza, P; C.M.\n",
      "King\tin\tKing\n",
      "Erly\tin\tBarbosa, M; Erly\n",
      "Edna\tin\tEdna\n",
      "Enio\tin\tMaria; Roberto; Sandro; Enio\n",
      "Gunn\tin\tGunn\n",
      "S.B.\tin\tBueno; S.B.\n",
      "Nilo\tin\tPires, JM; Black; Wurdack, J; Nilo\n",
      "Moro\tin\tMoro\n",
      "Feep\tin\tFeep\n",
      "Maia\tin\tMaia; Alcina\n",
      "A.M.\tin\tA.M.\n",
      "Ivan\tin\tIvan\n",
      "Wall\tin\tWall\n",
      "Sena\tin\tSena\n",
      "M.G.\tin\tBarbosa; M.G.\n",
      "F.J.\tin\tQuintiliano; F.J.; Colvéquia; L.P.T; Silva; D.R.\n",
      "Lelo\tin\tUrbanetz, C; Mariuza; Domingos; Lelo\n",
      "R.M.\tin\tGodfrey, RK; Tryon Jr.; R.M.\n",
      "Mota\tin\tCid; Ramos; Mota; Rosas\n",
      "João\tin\tFaria, I; Eliane; João\n",
      "E.M.\tin\tRibeiro, JELS; Ramos, JF; Santana f.; E.M.; Souza, SS\n",
      "Ipse\tin\tIpse\n",
      "L, D\tin\tSmith, LB; Magnanini, A; Silva, SLO; L, D\n",
      "Tmex\tin\tTmex\n",
      "JRBM\tin\tJRBM\n",
      "Luiz\tin\tLuiz\n",
      "Luís\tin\tLuís\n",
      "Fiek\tin\tFiek\n",
      "Melo\tin\tMelo; França\n",
      "UFJF\tin\tUFJF\n",
      "Wong\tin\tWong\n",
      "D.R.\tin\tSilva; D.R.; Colvéquia; L.P.T\n",
      "Neto\tin\tNeto\n",
      "Lóla\tin\tMeneguzzo, TEC; Ibrahim, M; Faria, JEQ; Caires, CS; Campos, LZO; Martins, RC; Lóla\n",
      "E.H.\tin\tE.H.\n",
      "Adão\tin\tAdão\n",
      "UFRJ\tin\tUFRJ\n",
      "Caio\tin\tPaula, A; Maria; Gabriel; Caio; William\n",
      "M.P.\tin\tM.P.\n",
      "Otto\tin\tMeneguzzo, TEC; Ibrahim, M; Faria, JEQ; Caires, CS; Campos, LZO; Martins, RC; Otto\n",
      "Senna\tin\tSenna; et al.\n",
      "U.M.R\tin\tU.M.R\n",
      "Adler\tin\tBlack, GA; Adler\n",
      "Burch\tin\tBurch\n",
      "Walte\tin\tRatter, JA; Cardoso, E; Fonseca Filho, J; Lima, V; Oliveira, N; Paixão, JF; Silva, M; Walte\n",
      "Elias\tin\tElias; Windler, DR\n",
      "M.F.A\tin\tM.F.A\n",
      "Lu, L\tin\tShevock, JR; Lenz, M; Nelson, J; Lu, L; Fritsch, P\n",
      "Ho, B\tin\tCâmara, PEAS; Tan, BC; Ho, B\n",
      "André\tin\tAndré; et al.\n",
      "Schio\tin\tSchio; et al.\n",
      "Matos\tin\tMatos; Conceição, GM\n",
      "Jairo\tin\tSilva, FC; Jairo; Wilson; Santos, GF\n",
      "L.P.T\tin\tSilva; D.R.; Colvéquia; L.P.T\n",
      "Lucas\tin\tLucas\n",
      "Aline\tin\tAline\n",
      "Eriel\tin\tNovelino, RF; Eriel\n",
      "Blotz\tin\tBlotz\n",
      "Young\tin\tYoung\n",
      "Vasco\tin\tVasco\n",
      "Gates\tin\tGates, B; Gates; Estabrook, GF\n",
      "Jonny\tin\tConrado; Jonny\n",
      "Maria\tin\tPaula, A; Maria; Gabriel; Caio; William\n",
      "Leise\tin\tLeise; et al.\n",
      "Passo\tin\tRatter, JA; Bridgewater, S; Cardoso, ES; Fonseca, J; Lima, V; Passo\n",
      "Silva\tin\tSilva; D.R.; Colvéquia; L.P.T\n",
      "Paulo\tin\tPaulo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = sorted( getNamesList(occs['recordedBy_atomized']) , key=lambda x: len(x))[:100]\n",
    "l = [ (n, getRecordsBy(occs, 'recordedBy_atomized', n)['recordedBy'].iloc[0]) for n in l ]\n",
    "print(\"Shortest names\")\n",
    "print(\"name\\tin\\tnamestring\")\n",
    "print(\"==========================\")\n",
    "print(''.join( \"{}\\tin\\t{}\\n\".format(n,nstr)for n,nstr in l ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest names\n",
      "=============\n",
      "Organografia e Sistemática das Fanerofíticas (A) - UnB\n",
      "Estudantes da Faculdade de Agronomia Eliseu Maciel\n",
      "Equipe da CHESF CT nº 1.92.2004.0150.00/PETCON\n",
      "Morfologia e Taxonomia de Fanerógamas - UnB\n",
      "Taxonomy Class of Universidade de Brasília\n",
      "Taxonomy Class of Universidade de Brasíl\n",
      "Equipe do Jardim Botânico de Brasília\n",
      "Curso int. Flora Páramo de Chirripó\n",
      "Stud. biol. Rheno-Trai. in itinere\n",
      "Turma de Etnobotânica do Cerrado\n",
      "Romero-Silva M.G. (D. Mariinha)\n",
      "Projeto Flora \"Pedra do Cavalo\"\n",
      "III Reunião de Bot. Peninsular\n",
      "Ecologia Vegetal Polo Noroeste\n",
      "Souza e Silva (D. Cezarina), C\n",
      "Alunos de Sistemática Vegetal\n",
      "Turma de Vegetação do Cerrado\n",
      "Sr. Air, Sr. Milton, Rodrigo\n",
      "Universidade de Brasília-UnB\n",
      "Disciplina Taxonomia Vegetal\n",
      "Glenfield Vet. Res. Station\n",
      "Study Biological Rheno-Trai\n",
      "Perth City Council Gardener\n",
      "Turma de Botânica de Campo\n",
      "Pessoal do Herbário da UnB\n",
      "Pessoal do Horto Florestal\n",
      "Pessoal do Jardim Botânico\n",
      "Projeto Biodiversidade BP\n",
      "Turma de Mestrado da UFBA\n",
      "Disciplina Taxonomia Vege\n",
      "Gomes (D. Sebastiana), S\n",
      "Projeto Flora Cristalino\n",
      "Equipe Pantanal - Poconé\n",
      "Mrs. R. L. Otto-Surbeck\n",
      "Sodré (D. Leonidia), LC\n",
      "Bastos (D. Preta), MAG\n",
      "Grupo de Kew Herbarium\n",
      "De-Lamanica-Freire, EM\n",
      "Florschütz de Waard, J\n",
      "Flora da E.E. do Panga\n",
      "Vesey-FitzGerald, LDEF\n",
      "Nascimento Júnior, JE\n",
      "Aparecida da Silva, M\n",
      "Nascimento Júnior, IC\n",
      "Grupo Flora P. Cavalo\n",
      "Carvalho-Sobrinho, JG\n",
      "Alunos de Biologia IV\n",
      "1980 Sino-Amer Exped.\n",
      "Rodrigues-da-Silva, R\n",
      "Maas Geesteranus, RA\n",
      "Gemtchújnicov, ID de\n",
      "Savassi-Coutinho, AP\n",
      "Kirkbride Junior, JH\n",
      "Cárdenas Soriano, MA\n",
      "Morais (D. Nina), JR\n",
      "Fernández-Alonso, JL\n",
      "Pereira L. (1983), L\n",
      "Fank-de-Carvalho, SM\n",
      "Acevedo-Rodriguez, P\n",
      "Mendes (D. Zefa), MS\n",
      "Damasceno Júnior, GA\n",
      "Barbosa Rodrigues, S\n",
      "Alcoforado Filho, FG\n",
      "The manager, lever's\n",
      "Rodrigues Júnior, CE\n",
      "Kinoshita-Gouvea, LS\n",
      "Gorts-van Rijin, ARA\n",
      "Serafim Sobrinho, J\n",
      "Pereira-Noronha, MR\n",
      "Kirkbride Junior, J\n",
      "Vasconcelos Neto, J\n",
      "Magalhães Silva, MR\n",
      "Rutter-Drummond, RA\n",
      "Monteiro Jr. Borges\n",
      "Engevix SA - BSB/BH\n",
      "Redfearn Junior, PL\n",
      "Coleção viva do IRI\n",
      "Capellari Júnior, L\n",
      "Coimbra Júnior, CEA\n",
      "Cavalcante-Silva, D\n",
      "Carqueira-Filho, FM\n",
      "Marie-Victorin, JLC\n",
      "Oliveira Filho, IMX\n",
      "Franceschinelli, EV\n",
      "Almeida-Scabbia, RJ\n",
      "Gontijo Júnior, JAB\n",
      "Graciano-Ribeiro, D\n",
      "Teixeira Júnior, AQ\n",
      "Lamônica Freire, ME\n",
      "Ribeiro-Júnior, GG\n",
      "Rolland-Germain, F\n",
      "Rodrigues Filho, J\n",
      "Espírito Santo, FS\n",
      "Lanna Sobrinho, JP\n",
      "Disciplina Princíp\n",
      "Vieira Júnior, WCJ\n",
      "Guimarães Neto, FM\n",
      "Parucker Junior, N\n",
      "Pinho-Ferreira, MA\n",
      "Von Humboldt, FWHA\n"
     ]
    }
   ],
   "source": [
    "l = sorted( getNamesList(occs['recordedBy_atomized']) , key=lambda x: len(x), reverse=True)[:100]\n",
    "print(\"Longest names\\n=============\")\n",
    "print('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the UB dataset has a high-quality collection authorship field, I've only found few inconsistencies, which are sufficient to show how this validation approach works. For lower quality datasets, though, this validation would be more time consuming. To exemplify, let's take a look at collector 'Yushun.;K', which I presume should have been recorded as 'Yushun, K'. As the `';'` separator was misused for separating her/his last name from the initial, our default atomization function creates two entities from this string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Yushun.', 'K.']], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_record_by_yushun = lambda x: x[x['recordedBy'].apply(lambda n: 'Yushun' in n)]\n",
    "get_record_by_yushun(occs)['recordedBy_atomized'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below I define the replacement list and renormalize the `recordedBy` column, now using the `replaces` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep = [ \n",
    "    (['Sr. Air, Sr. Milton, Rodrigo'], \"Sr. Air; Sr. Milton; Rodrigo\"),\n",
    "    (['Sônia / Josefina'], \"Sônia; Josefina\"),\n",
    "    (['Hatschbach, G; M.'], \"Hatschbach, G; Hatschbach, M\"),\n",
    "    (['Irwin, HS; Souza, R; Santos; RR'], \"Irwin, HS; Souza, R; Santos, RR\"),\n",
    "    (['Kirkbride Junior, JH; Ono; E.K.M; et al.'], \"Kirkbride Junior, JH; Ono, E.K.M; et al.\"),\n",
    "    (['Carboni, M; Faraco, AG; Soares; P.G.; Sampaio, D; Breier, TB'], \"Carboni, M; Faraco, AG; Soares, P.G.; Sampaio, D; Breier, TB\"),\n",
    "    (['Silva; D.R.; Colvéquia; L.P.T'], \"Silva, D.R.; Colvéquia, L.P.T\"),\n",
    "    (['Quintiliano; F.J.; Colvéquia; L.P.T; Silva; D.R.'], \"Quintiliano, F.J.; Colvéquia, L.P.T; Silva, D.R.\"),\n",
    "    \n",
    "    (['Yushun.; K.', \n",
    "      'Barbosa; M.G.',\n",
    "      'Hällström; E.',\n",
    "      'Bueno; S.B.'], \n",
    "            lambda x: x.replace(';',',')\n",
    "    ),   \n",
    "]\n",
    "\n",
    "atomizingFunction = lambda x: nc.namesFromString(x,delim=';') \n",
    "atomized_recordedBy = atomizeNames(occs['recordedBy'],operation=atomizingFunction, replaces=rep)\n",
    "occs['recordedBy_atomized'] = atomized_recordedBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have recreated the `recordedBy_atomized` field with the exceptions defined above. Note that now 'Yushun.; K.' is included as a single entity in the `recordedBy_atomized` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Yushun., K.']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_record_by_yushun(occs)['recordedBy_atomized'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll create the names map, which deals with the issue of heteronymous entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building names map and index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since last notebook I have updated some of the `NamesMap` class' methods, while also adding new ones. Now the names map object can be saved into a *.json* file, which can be shared with other potential users of the dataset. This simplifies the names cleaning and validation process and allows it to be performed collaboratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "class NamesMap:\n",
    "    _map=None\n",
    "    _normalizationFunction=None\n",
    "    _remappingDict=None\n",
    "    \n",
    "    def __init__(self, names, normalizationFunction, *args, **kwargs):  \n",
    "        self._normalizationFunction = normalizationFunction    \n",
    "        if 'jsondata' in kwargs:\n",
    "            self._map = kwargs['jsondata']['_map']\n",
    "            self._remappingDict = kwargs['jsondata']['_remappingDict']\n",
    "        else:\n",
    "            self._map = dict( (n, self._normalizationFunction(n)) for n in names )\n",
    "        return\n",
    "    \n",
    "    def clearMap(self):\n",
    "        \"\"\"\n",
    "        Resets the map to an empty dict\n",
    "        \"\"\"\n",
    "        self._map={}\n",
    "        return\n",
    "    \n",
    "    def remap(self, remappingDict, fromScratch=False, preventOverwriting=True):\n",
    "        \"\"\"\n",
    "        Updates the remapping dictionary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        remappingDict : dict\n",
    "            Remaps values in the map attribute.\n",
    "        \n",
    "        fromScratch : bool\n",
    "            If set to True the remapping dict becomes the one passed in. All other previous\n",
    "            remaps are discarded.\n",
    "        \n",
    "        preventOverwriting : bool\n",
    "            If set to True (default), then a key cannot be remapped if it already exists\n",
    "            in the remapping dict.\n",
    "        \"\"\"\n",
    "        remappingDict = deepcopy(remappingDict)\n",
    "        if fromScratch:\n",
    "            self._remappingDict = None\n",
    "            \n",
    "        if self._remappingDict is None:\n",
    "            self._remappingDict = remappingDict\n",
    "        \n",
    "        else:\n",
    "            if preventOverwriting:\n",
    "                existantKeys = self._remappingDict.keys()\n",
    "                for k in remappingDict.keys():\n",
    "                    if k in existantKeys:\n",
    "                        raise ValueError(\"Cannot overwrite key '{}' in the remapping dict.\".format(k))\n",
    "                    \n",
    "            self._remappingDict.update(remappingDict)\n",
    "        return    \n",
    "    \n",
    "    def remove_fromRemap(self, key):\n",
    "        \"\"\"\n",
    "        Removes element from the remapping dict\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Key : dict key\n",
    "          Key of the element to be removed\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "        The value associated to the removed key\n",
    "        \"\"\"\n",
    "        return self._remappingDict.pop(key)\n",
    "\n",
    "    def remap_fromJson(self, filepath, fromScratch=True):\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            remappingDict = data['_remappingDict']\n",
    "            self.remap(remappingDict,fromScratch)\n",
    "            return\n",
    "    \n",
    "    def setNormalizationFunc(self,normalizationFunction):\n",
    "        self._normalizationFunction = normalizationFunction\n",
    "        return\n",
    "    \n",
    "    def insertNames(self, names, normalizationFunction=None, rebuild=False):\n",
    "        # if rebuild is true, the entire map (but not the remapping dict) is rebuilt from scratch\n",
    "        if rebuild==True:\n",
    "            self.clearMap()\n",
    "        if normalizationFunction is None: \n",
    "            if self._normalizationFunction is None:\n",
    "                raise ValueError(\"a normalization function must be defined\")\n",
    "            normalizationFunction = self._normalizationFunction\n",
    "        self._map.update( dict( (n,normalizationFunction(n)) for n in names ) )\n",
    "        return\n",
    "    \n",
    "    def getMap(self, remap=True):\n",
    "        \"\"\"\n",
    "        Returns a COPY of the names dictionary\n",
    "        If remap is set to True the remapping dictionary\n",
    "        is used to remap some names.\n",
    "        \"\"\"\n",
    "        res = deepcopy(self._map)\n",
    "        if remap and self._remappingDict is not None:\n",
    "            getNamesPrimitives = lambda n: ( name for name,norm in self._map.items() if norm == n )\n",
    "            for n,t in ( (n,t) for s,t in self._remappingDict.items() for n in getNamesPrimitives(s) ):\n",
    "                res[n]=t\n",
    "        return res\n",
    "    \n",
    "    def getNormalizedNames(self, remap=True):\n",
    "        return sorted(list(set(self.getMap(remap=remap).values())))\n",
    "    \n",
    "    def getNamePrimitives(self, n):\n",
    "        nmap = self.getMap()\n",
    "        return [ name for name,norm in nmap.items() if norm == n ]\n",
    "    \n",
    "    def write_toJson(self, filename=\"names_map.json\"):\n",
    "        json_dict = dict( (k,v) for (k,v) in vars(self).items() if k!='_normalizationFunction')\n",
    "        with open(filename, 'w') as output_file:\n",
    "            json.dump( json_dict, output_file, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "        return\n",
    "    \n",
    "    def reportRemappingInconsistencies(self, returnFormatted=True):\n",
    "        \"\"\"\n",
    "        Reports inconsistencies in the remapping dictionary. Possible inconsistencies\n",
    "        classes are:\n",
    "          1. Keys that also appear as values;\n",
    "        \"\"\"\n",
    "        inconsistencies_dict = {'keys_in_vals':('These names appear both as keys and values:', [])}\n",
    "        dictKeys = self._remappingDict.keys()\n",
    "        dictVals = self._remappingDict.values()\n",
    "        \n",
    "        # Keys_in_vals inconsistency\n",
    "        for k in dictKeys:\n",
    "            if k in list(dictVals):\n",
    "                inconsistencies_dict['keys_in_vals'][1].append(k)\n",
    "        \n",
    "        if sum( len(vals) for k,(desc,vals) in inconsistencies_dict.items() )==0:\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            if returnFormatted:\n",
    "                resStr = \"\"\n",
    "                for k,(desc, vals) in inconsistencies_dict.items():\n",
    "                    if len(vals)!=0:\n",
    "                        resStr += \"{}\\n{}\\n\".format(desc, ''.join(\"=\" for i in range(len(desc))) )\n",
    "                        resStr += ''.join( \"  {}\\n\".format(v) for v in vals )\n",
    "                        resStr += \"{}\\n\".format(''.join(\"-\" for i in range(len(desc))) ) \n",
    "                    return resStr\n",
    "\n",
    "            else:\n",
    "                return inconsistencies_dict\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def read_namesMap(filepath, fileType='json', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a names map from a file and returns a NamesMap instance.\n",
    "    Currently only json files are supported.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The normalization function cannot be stored in JSON, and therefore it \n",
    "    must be passed as an optional keyword argument 'normalizationFunction'. If no \n",
    "    normalization function is passed new names cannot be inserted into the map,\n",
    "    although remapping can still be done.\n",
    "    \"\"\"\n",
    "    if fileType=='json':\n",
    "        with open(filepath, 'r') as f:\n",
    "            data=json.load(f)\n",
    "            nm = NamesMap(names=None, normalizationFunction=kwargs.get('normalizationFunction', None), jsondata=data)\n",
    "        return nm\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type '{}'.\".format(fileType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate the names map from the collectors names list and then the names index from the map. I'll use the `normalize` function from my cleaning module `nc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nl = getNamesList(occs['recordedBy_atomized'], orderBy=\"alphabetic\")\n",
    "nm = NamesMap(nl, nc.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ni = nc.getNamesIndexes(occs, 'recordedBy_atomized',namesMap=nm.getMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assembling the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use `CoworkingNetwork` class to create my network model. It will be further use to help finding heteronymous collectors, by using a nodes neighborhood connectivity metric. This is a cyclic process, as remapped nodes can be then used to rebuild the network model, which again can be used to indicate new possible associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "class CoworkingNetwork(networkx.Graph):\n",
    "    \"\"\"\n",
    "    Class for coworking networks. Extends networkx Graph class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    namesSets : iterable\n",
    "        An iterable of iterables containing names used to compose cliques \n",
    "        in the network.\n",
    "\n",
    "    weighted : bool\n",
    "        If set to True the resulting network will have weighted edges. Default False.\n",
    "        \n",
    "    namesMap : NamesMap\n",
    "        A NamesMap object for normalizing nodes names.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> namesSets = [ ['a','b','c'], ['d','e'], ['a','c'] ]\n",
    "    >>> CoworkingNetwork( namesSets, weighted=True).edges(data=True)\n",
    "    [('b', 'a', {'weight': 1}),\n",
    "     ('b', 'c', {'weight': 1}),\n",
    "     ('a', 'c', {'weight': 2}),\n",
    "     ('e', 'd', {'weight': 1})]\n",
    "    \n",
    "    >>> CoworkingNetwork( namesSets ).edges(data=True)\n",
    "    [('b', 'a', {}), \n",
    "     ('b', 'c', {}), \n",
    "     ('a', 'c', {}), \n",
    "     ('e', 'd', {})]\n",
    "    \"\"\"\n",
    "    def __init__(self, namesSets, weighted=False, namesMap=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if namesMap:\n",
    "            nmap = namesMap.getMap()\n",
    "            namesSets = [ [ nmap[n] for n in nset ] for nset in namesSets ]\n",
    "            \n",
    "        cliques = map( lambda n: itertools.combinations(n,r=2), namesSets )\n",
    "        edges = [ e for edges in cliques for e in edges ]\n",
    "        self.add_edges_from(edges)\n",
    "        \n",
    "        if weighted:\n",
    "            edges_weights = Counter(edges)\n",
    "\n",
    "            for (u,v),w in edges_weights.items():\n",
    "                try:\n",
    "                    self[u][v]['weight'] += w\n",
    "                except:\n",
    "                    self[u][v]['weight'] = w\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nm.getMap()\n",
    "ni = nc.getNamesIndexes(occs, 'recordedBy_atomized',namesMap=nm.getMap())\n",
    "G = CoworkingNetwork( occs['recordedBy_atomized'], weighted=True, namesMap=nm )\n",
    "G.remove_node('etal')\n",
    "nx.write_gexf(G,'graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, 'n_records', dict( (n, len(ni[n])) for n in G.nodes() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Looking for heteronymous entities in the network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of identity for collectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first attempt to design a metric for finding potential heteronymous entities (entities with multiple name variations) based on their neighborhood connectivity. It is composed by two main steps:\n",
    "1. Find candidate names to be referring to common entities using a **name sequence matching algorithm**;\n",
    "2. For close matches, calculate the **likelihood** of pairs of nodes being the same entity based on their neighbors. A first, simplistic likelihood function is defined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$likelihood(n_h, n_l) = \\begin{cases} \\frac{1}{k_l}  \\sum_{i=1}^{k_l} g(v_i) \\quad  n_l \\notin S_h\\\\\n",
    "0 \\quad  n_l \\in S_h \\end{cases}, \\quad \n",
    "g(x) = \\begin{cases} 1 \\quad x \\in S_h \\\\ 0 \\quad x \\notin S_h \\end{cases}, \\qquad \\textit{where}$$\n",
    "\n",
    "* $n_h$ is the higher-degree node;\n",
    "* $n_l$ is the lower-degree node;\n",
    "* $k_h$ is the degree of node $n_h$;\n",
    "* $k_l$ is the degree of node $n_l$;\n",
    "* $v_i$ are neighbors of node $n_l$;\n",
    "* $S_h$ is the set of neighbors of node $n_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this likelihood metric is described by the following rules:\n",
    "1. The likelihood that two names refer to the same entity depends on the percentage of neighbors they share.\n",
    "2. If two candidate names are both included in any record then they're neighbors in the coworking network. This indicates that two entities holding each of these names have in fact co-authored at least one record, and they are therefore discarded as candidates of referring to a single entity. The likelihood score is set to be zero;\n",
    "3. A lower-degree candidate sharing a high percentage of neighbors with a higher-degree candidate is more likely to be the same entity (already considering they've not co-authored any records).\n",
    "4. Isolated nodes (with zero degree) are automatically excluded from candidate identities. \n",
    "\n",
    "I initially had considered using the [*cosine similarity*](https://en.wikipedia.org/wiki/Cosine_similarity) metric for comparing nodes neighborhoods, but it turned out to disregard many relevant identities where the highest-degree node had considerably higher degree than the lowest-degree node. Many relevant identities have very different degrees, as in most cases they're derived from a typo when recording the collectors' names. Below I define the likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihoodOfIdentity( G, n1, n2 ):\n",
    "    if len(G[n1]) > len(G[n2]):\n",
    "        n_h,n_l = n1,n2\n",
    "    else:\n",
    "        n_h,n_l = n2,n1\n",
    "    \n",
    "    S = G.neighbors(n_h)\n",
    "    k_h = G.degree(n_h)\n",
    "    k_l = G.degree(n_l)\n",
    "    \n",
    "    if n_l in S or k_l==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum( 1 if v in S else 0 for v in G.neighbors(n_l) )/k_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the likelihood function for collectors selected by the sequence matching algorithm. The result will be a list of tuples containing pairs of names and their associated likelihood score. This piece of code takes time to be executed, as each name must be compared against every other name, with a total of $n \\cdot(n-1)$ executions of the sequence matching algorithm. As a result we get a list with possible mappings, which must be inspected before being used to remap the NamesMap. This list is composed by 3-tuples $(n1,n2,S)$, where the name in $n1$ remaps to that in $n2$ and $S$ is the likelihood score, computed using the function defined in the cell above. It should be noted that $n1$ and $n2$ are tuples $(name,num_{records})$, such that $num_{records\\_n1}$ is always less than $num_{records\\_n2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib as dfl\n",
    "names = [ n for n,d in sorted( G.nodes(data=True), key=lambda x: x[1]['n_records'], reverse=True ) ]\n",
    "n_records_dict = dict( (n, d['n_records']) for n,d in G.nodes(data=True) )\n",
    "\n",
    "l=[]\n",
    "similarity_threshold=0.1\n",
    "for n1 in names:\n",
    "    for n2 in [ n2 for n2 in dfl.get_close_matches(n1, names) if n2!=n1 ]:\n",
    "        sim = likelihoodOfIdentity(G,n1,n2)\n",
    "        n1_num_records = n_records_dict[n1]\n",
    "        n2_num_records = n_records_dict[n2]\n",
    "        if sim >= similarity_threshold:\n",
    "            if n2_num_records > n1_num_records:\n",
    "                l += [ ((n1, n1_num_records), (n2, n2_num_records), sim) ]\n",
    "            else:\n",
    "                l += [ ((n2, n2_num_records), (n1, n1_num_records), sim) ]\n",
    "                \n",
    "l = list(set(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the list ordered by likelihood score followed by n2's num of records and n1's num of records. I'll only output the first 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('souza,mg', 5), ('souza,mgm', 4229), 1.0),\n",
       " (('munhoz,ca', 2), ('munhoz,cbr', 3932), 1.0),\n",
       " (('santos,rrb', 2), ('santos,rr', 3879), 1.0),\n",
       " (('harley,gm', 3), ('harley,rm', 3016), 1.0),\n",
       " (('belem,pr', 19), ('belem,rp', 2745), 1.0),\n",
       " (('kirkbridejunior,j', 13), ('kirkbridejunior,jh', 2423), 1.0),\n",
       " (('amorim,pr', 80), ('amorim,prf', 2231), 1.0),\n",
       " (('amorim,p', 1), ('amorim,prf', 2231), 1.0),\n",
       " (('pires,jn', 1), ('pires,jm', 1785), 1.0),\n",
       " (('fonseca,s', 57), ('fonseca,sf', 1779), 1.0),\n",
       " (('fonseca,fg', 1), ('fonseca,sf', 1779), 1.0),\n",
       " (('melo,trb', 243), ('mello,trb', 1529), 1.0),\n",
       " (('mendonca,rr', 1), ('mendonca,rc', 1441), 1.0),\n",
       " (('mendonca,fca', 1), ('mendonca,rc', 1441), 1.0),\n",
       " (('carvalho,avm', 2), ('carvalho,am', 1429), 1.0),\n",
       " (('onishi', 4), ('onishi,e', 1352), 1.0),\n",
       " (('onishi,gl', 1), ('onishi,e', 1352), 1.0),\n",
       " (('philcox', 1), ('philcox,d', 1322), 1.0),\n",
       " (('souza,rv', 67), ('sousa,rv', 1291), 1.0),\n",
       " (('villar,ts', 4), ('villarroel,d', 1219), 1.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(l, key=lambda x: (x[2], x[1][1], x[0][1]),reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can then inspect candidate identities and build a remapping dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remap = {\n",
    "    'xavier,s': 'xavier,scs',\n",
    "    'kinoshita,ls': 'kinoshitagouvea,ls',\n",
    "    'teodoro,d': 'teodoro,daa',\n",
    "    'leoni,l': 'leoni,ls',\n",
    "    'krieger,l': 'krieger,pl',\n",
    "    'dutilh,j': 'dutilh,jha',\n",
    "    'arzolla,fardp': 'arzolla,farp',\n",
    "    'shepherd,g': 'shepherd,gj',\n",
    "    'caliari,cp': 'calliari,cp',\n",
    "    'parente,hmv': 'parente,hvm',\n",
    "    'parente,hm': 'parente,hvm',\n",
    "    'ledoux': 'ledoux,p',\n",
    "    'montefusco,n': 'montefuso,neg',\n",
    "    'bovini,m': 'bovini,mg',\n",
    "    'kuhlmann,m': 'kuhlmann,mp',\n",
    "    'cordovil,s': 'cordovil,sp',\n",
    "    'salas,r': 'salas,rm',\n",
    "    'kallunki,j': 'kallunki,ja',\n",
    "    'ianhez,m': 'ianhez,ml',\n",
    "    'somavilla,n': 'somavilla,nsd',\n",
    "    'welle,bjh': 'terwelle,bjh', \n",
    "    'giulietti,an': 'giulietti,aml',\n",
    "    'barboza,e': 'barbosa,e', \n",
    "    'juchum,f': 'juchum,fs',\n",
    "    'bianchetti,l': 'bianchetti,lb',\n",
    "    'melo,mrf': 'melo,mmrf',\n",
    "    'rosario,c': 'rosario,cs',\n",
    "    'damasceno,ga': 'damascenojunior,ga',\n",
    "    'custodio,a': 'custodiofilho,a',\n",
    "    'salimena,f': 'salimena,frg',\n",
    "    'chen,c': 'chen,ch',\n",
    "    'rocha,d': 'rocha,ds',\n",
    "    'matos,m': 'matos,mq',\n",
    "    'assumpcao,s': 'assuncao,s',\n",
    "    'santos,hg': 'santos,hgp',\n",
    "    'bonomo': 'bonomo,vs',\n",
    "    'fonseca,j': 'fonseca,js',\n",
    "    'anderson,w': 'anderson,wr',\n",
    "    'kollmann,ljc': 'kollmann', \n",
    "    'kollmann,rl': 'kollmann',\n",
    "    'pabst,g': 'pabst,gfj',\n",
    "    'borges,l': 'borges,lm',\n",
    "    'baungarten,j': 'baumgarten,j',\n",
    "    'ferraz,nm': 'ferraz,nms',\n",
    "    'tressens,s': 'tressens,sg',\n",
    "    'moreira,m': 'moreira,mv',\n",
    "    'gifford,d': 'gifford,dr',\n",
    "    'rosa,n': 'rosa,na',\n",
    "    'giulietti,am': 'giulietti,aml',\n",
    "    'taxonomyclassofuniversidadedebrasil': 'taxonomyclassofuniversidadedebrasilia',\n",
    "    'santos,b': 'santos,br',\n",
    "    'schettino': 'schettino,vm',\n",
    "    'mattos,lam': 'mattossilva,la',\n",
    "    'gomes,vl': 'klein,vlg',\n",
    "    'amorim,a': 'amorim,ah',\n",
    "    'jardim,a': 'jardim,ab',\n",
    "    'lima,jc': 'lima,jcm',\n",
    "    'barros,m': 'barros,mag',\n",
    "    'coelho,d': 'coelho,df',\n",
    "    'forster,w': 'foster,w',\n",
    "    'kozovits,a': 'kozovits,ar',\n",
    "    'occhioni,p': 'ochioni,p',\n",
    "    'brugger,m': 'brugger,mc',\n",
    "    'moreira,h': 'moreira,hjc',\n",
    "    'egler,w': 'egler,wa',\n",
    "    'pott,v': 'pott,vj',\n",
    "    'lima,am': 'lima,amb',\n",
    "    'graziela': 'graziella,m',\n",
    "    'guimaraes,p': 'guimaraes,pjf',\n",
    "    'bueno,n': 'bueno,nc',\n",
    "    'werneck,w': 'werneck,wl',\n",
    "    'senna,l': 'senna,lr',\n",
    "    'ribeira,sc': 'ribeiro,sc',\n",
    "    'lin,h': 'lin,hh',\n",
    "    'james,t': 'james,ta',\n",
    "    'salimena,fr': 'salimena,frg',\n",
    "    'mori,s': 'mori,sa',\n",
    "    'silva,jc': 'silva,jcs',\n",
    "    'ramos': 'ramos,r',\n",
    "    'santana,s': 'santana,sc',\n",
    "    'amaral,a': 'amaral,ag',\n",
    "    'correa,c': 'correia,c',\n",
    "    'grear,jw': 'grearjunior,jw',\n",
    "    'harley,r': 'harley,rm',\n",
    "    'oliveira,p': 'oliveira,pp',\n",
    "    'gracianoribeiro,d': 'ribeiro,dg',\n",
    "    'landrum,l': 'landrum,lr',\n",
    "    'oliveira,n': 'oliveira,nro',\n",
    "    'stannard,b': 'stannard,bl',\n",
    "    'amorim,am': 'amorim,ama',\n",
    "    'gomesklein,vl': 'klein,vlg',\n",
    "    'lorenco,jlm': 'lourenco,jml',\n",
    "    'lourenco,jlm': 'lourenco,jml',\n",
    "    'rosini,j': 'rossini,j',\n",
    "    'mattos,la': 'mattossilva,la',\n",
    "    'lima,v': 'lima,vp',\n",
    "    'gomes,l': 'gomes,lcj',\n",
    "    'pires,f': 'pires,fs',\n",
    "    'king,lrm': 'king,rm',\n",
    "    'santos,j': 'santos,jal',\n",
    "    'prado,al': 'prado,ajl', \n",
    "    'judziewicz,e': 'judziewicz,ej',\n",
    "    'buzzi,f': 'bucci,ffb',\n",
    "    'walter': 'walter,bmt',\n",
    "    'klein,r': 'klein,rm',\n",
    "    'klein': 'klein,rm',\n",
    "    'fazza,fa': 'fazza,lfa',\n",
    "    'noris,d': 'norris,d',\n",
    "    'mota,cd': 'mota,cda',\n",
    "    'silva,pe': 'silva,pen',\n",
    "    'edsonchaves,b': 'chaves,be', \n",
    "    'santana,me': 'santanna,me',\n",
    "    'sanaiot': 'sanaiotti,tm',\n",
    "    'marques,mc': 'marques,mcm',\n",
    "    'bresolin': 'bresolin,a',\n",
    "    'wurdack,j': 'wurdack,jj',\n",
    "    'borgato,d': 'borgatto,df',\n",
    "    'paixao,j': 'paixao,jl',\n",
    "    'ferreira,j': 'ferreirapaixao,j',\n",
    "    'niclughada,em': 'lughadha,en',\n",
    "    'rocha,w': 'rocha,wd',\n",
    "    'cruz,t': 'cruz,ta',\n",
    "    'araujo,d': 'araujo,da',\n",
    "    'carvalho,amv': 'carvalho,avm',\n",
    "    'deslacio,f': 'delascio,jf',\n",
    "    'fagg,wc': 'fagg,cw',\n",
    "    'haas,h': 'hass,jh',\n",
    "    'faria,ca': 'farias,ca',\n",
    "    'fonseca,s': 'fonseca,sc',\n",
    "    'zolma,fj': 'zelma,fj', \n",
    "    'sidnei,rm': 'sidney',\n",
    "    'sidney,gf': 'sidney',\n",
    "    'gatti,g': 'gatti,j',\n",
    "    'sousa,rtc': 'souza,rtc',\n",
    "    'vicente,j': 'vicente,jc',\n",
    "    'hind,p': 'hind,pd',\n",
    "    'dunaiski,a': 'dunaiskijunior,a',\n",
    "    'souza,mg': 'souza,mgm',\n",
    "    'cardoso,cf': 'cardoso,cfr',\n",
    "    'fernandes,gd': 'fernandes,gdf',\n",
    "    'meireles,ml': 'meirelles,ml',\n",
    "    'siqueira,g': 'siqueira,gs',\n",
    "    'capelo,fa': 'capelo,ffm',\n",
    "    'mota,da': 'mota,cda',\n",
    "    'soares,lgs': 'soareslima,gs',\n",
    "    'aguiar,am': 'aguiar,an',\n",
    "    'wagner,nl': 'wagner,hl',\n",
    "    'anderson,l': 'anderson,le',\n",
    "    'veloso,lj': 'veloso,lm',\n",
    "    'smith,gl': 'smith,gm',\n",
    "    'abreu,lc': 'abreu,lcr',\n",
    "    'abreu,m': 'abreu,mc',\n",
    "    'abreu,ms':'abreu,mc',\n",
    "    'aguiar,ac': 'aguiar,aca',\n",
    "    'allem,a':'allem,ac',\n",
    "    'gawryszewski,fm': 'grawryszewski,fm',\n",
    "    'coradin,l': 'coradin,lc',\n",
    "    'jennings,': 'jennings,lvs',\n",
    "    'kolmann,l': 'kollmann,l',\n",
    "    'vera,l': 'veralucia',\n",
    "    'clemente,c': 'clemente,cm',\n",
    "    'bertolda,j': 'bertoldo,j',\n",
    "    'smith,g': 'smith,gm',\n",
    "    'vaz,a': 'vaz,amsf',\n",
    "    'sena,l': 'senna,lr',\n",
    "    'sanaiotti,t': 'sanaiotti,tm',\n",
    "    'klein,vl': 'klein,vlg',\n",
    "    'casto,ws': 'castro,ws',\n",
    "    'dias,ej': 'dias,jb',\n",
    "    'torres,dc': 'torres,dsc',\n",
    "    'landim,m': 'landim,mf',\n",
    "    'silva,lh': 'soares-silva,lh', \n",
    "    'soaressilva,lh': 'soares-silva,lh',\n",
    "    'silva,lhs': 'soares-silva,lh',\n",
    "    'oliveira,ma': 'oliveira,ms',\n",
    "    'borges,r': 'borges,rax',\n",
    "    'oliveira,s': 'oliveira,scc',\n",
    "    'lage,jl': 'hage,jl',\n",
    "    'maas,h': 'maas,pjm',\n",
    "    'cardoso,e': 'cardoso,es',\n",
    "    'proenca,c': 'proenca,ceb',\n",
    "    'noleto,l': 'noletto,lg',\n",
    "    'rudall,p': 'ruddall,p',\n",
    "    'chiea,sac': 'chiea,sc',\n",
    "    'cielofilho,r':'cielo-filho,r', \n",
    "    'filho,rc':'cielo-filho,r',\n",
    "    'cid,ca': 'cid,cac',\n",
    "    'nascimento,e': 'nascimento,ea',\n",
    "    'jardim,j': 'jardim,jg',\n",
    "    'villaroel,d': 'villarroel,d',\n",
    "    'wagner': 'wagner,hl',\n",
    "    'dias,bj': 'dias,jb',\n",
    "    'flores': 'flores,tb',\n",
    "    'lucas,e': 'lucas,ej',\n",
    "    'morbeck': 'morbeck,a',\n",
    "    'castro,r': 'castro,ra',\n",
    "    'passon,l': 'passon,lm',\n",
    "    'simpson,pl': 'simpson-junior,pl', \n",
    "    'simpsonjunior,pl': 'simpson-junior,pl',\n",
    "    'paulasouza,j': 'souza,jp',\n",
    "    'coveny,r': 'coveny,rg',\n",
    "    'crosby': 'crosby,mr',\n",
    "    'souza,cv': 'souza,vc',\n",
    "    'moreira,al': 'moreira,alc',\n",
    "    'nobs,ma': 'noles,ma',\n",
    "    'kuehn,e': 'kuhn,e',\n",
    "    'davidsen,c': 'davidson,c',\n",
    "    'estabrook': 'estabrook,gf',\n",
    "    'sousa,tc': 'souza,tc',\n",
    "    'verwimp': 'verwimp,i',\n",
    "    'campos,jmf': 'campos,jmp',\n",
    "    'silva,lm': 'silva,lam',\n",
    "    'smith,l': 'smith,lb',\n",
    "    'yamomoto,m': 'yamamoto,m',\n",
    "    'verveloet,rr': 'vervloet,rr',\n",
    "    'rocha,rm': 'rocha,rn',\n",
    "    'oliveira,nr': 'oliveira,nro',\n",
    "    'haas,jh': 'hass,jh',\n",
    "    'whalen,a': 'whalen,aj',\n",
    "    'sa,spp': 'sa,spps',\n",
    "    'bensusan,n': 'bensusan,nr',\n",
    "    'borgato,df': 'borgatto,df',\n",
    "    'mendes,jn': 'mendes,jm',\n",
    "    'fontella,j': 'fontella,jp',\n",
    "    'staggemeier,vg': 'staggmeier,vg',\n",
    "    'campos,mtv': 'campos,mtva',\n",
    "    'benton,f': 'benton,fp',\n",
    "    'marchioni,jm': 'marchiori,jn',\n",
    "    'juchum': 'juchum,fs',\n",
    "    'peocopio,lc': 'procopio,lc',\n",
    "    'romeroc': 'romero,c', \n",
    "    'marques,c': 'marques,cf',\n",
    "    'cardoso,ef': 'cardoso,f',\n",
    "    'pereira,ba': 'pereira,bas',\n",
    "    'carvalho,sl': 'carvalho-leite,sl', \n",
    "    'carvalholeite,sl': 'carvalho-leite,sl',\n",
    "    'benedete': 'benedete,al',\n",
    "    'harley,gm': 'harley,rm',\n",
    "    'diasmelo,r': 'dias-melo,r', \n",
    "    'melo,rd': 'dias-melo,r',\n",
    "    'schiesinki': 'schiesinski,d',\n",
    "    'porto,jr': 'porto,jlr',\n",
    "    'argent,g': 'argent,gcg',\n",
    "    'argentgcgin': 'argent,gcg',\n",
    "    'rodri': 'rodrig',\n",
    "    'araujo,g': 'araujo,gm',\n",
    "    'carvalho,am': 'carvalho,avm',\n",
    "    'careno,s': 'carreno,s',\n",
    "    'mazine,f': 'mazine-capelo,ff', \n",
    "    'mazinecapelo,ff': 'mazine-capelo,ff',\n",
    "    'lopes,i': 'lopes,isn',\n",
    "    'irvine,gc': 'irwine,cg',\n",
    "    'taroda,n': 'tarroda,n',\n",
    "    'isejima': 'isejima,em',\n",
    "    'dario,f': 'dario,fr',\n",
    "    'onishi': 'onishi,e',\n",
    "    'reitz': 'reitz,pr',\n",
    "    'reitz,r': 'reitz,pr',\n",
    "    'devogel,ef': 'vogel,hvf',\n",
    "    'borges,j': 'borges,jwm',\n",
    "    'brade': 'brade,ac',\n",
    "    'nicacio': 'nicacio,jn',\n",
    "    'anapaula': 'ana-paula', \n",
    "    'paula,a': 'ana-paula',\n",
    "    'dell,d': 'odell,d',\n",
    "    'rondon': 'rondon,c',\n",
    "    'santos,h': 'santos,hcf',\n",
    "    'furla': 'furlan,a',\n",
    "    'grasser,g': 'grasser,ga',\n",
    "    'pedrosa,ma': 'pedroso,ma',\n",
    "    'pedrosa,n': 'pedrosa,ns',\n",
    "    'zoccoli,d': 'zoccoli,dm',\n",
    "    'arroyo,mtk': 'kallin-arroyo,mt', \n",
    "    'kallinarroyo,mt': 'kallin-arroyo,mt', \n",
    "    'brito,ic': 'britto,ic',\n",
    "    'degrande,da': 'grande,da',\n",
    "    'sales,sc': 'salles,sc',\n",
    "    'souza,r': 'souza,rr',\n",
    "    'guedes,j': 'guedes,jc',\n",
    "    'herlan,j': 'herlanio,j',\n",
    "    'nascimento,a': 'nascimento,ae',\n",
    "    'siva,ma': 'silva,ma',\n",
    "    'bucci,f': 'bucci,ffb',\n",
    "    'santana,bdi': 'santana,bid',\n",
    "    'giordano,lc': 'giordano,lcs',\n",
    "    'meyer': 'meyer,fs',\n",
    "    'franca': 'franca,f', \n",
    "    'koekemoer,m': 'koekomoer,m',\n",
    "    'souza,rt': 'souza,rtc',\n",
    "    'pereira,t': 'pereira,ta',\n",
    "    'jose,m': 'jose,maria',\n",
    "    'carmo,j': 'carmo,jj',\n",
    "    'fernandes,a': 'fernandes,ag',\n",
    "    'moraes,plr': 'moraes,prl',\n",
    "    'maia,w': 'maia,wd',\n",
    "    'martins,ca': 'martins,can',\n",
    "    'polite,l': 'politi,l',\n",
    "    'almeida,f': 'almeida,fc',\n",
    "    'borges,jw': 'borges,jwm',\n",
    "    'kuhlman,m': 'kuhlmann,mp',\n",
    "    'silva,mb': 'silva,mib', \n",
    "    'sousa,rv': 'souza,rv',\n",
    "    'koczichi': 'koczicki,c',\n",
    "    'leite,jr': 'leite,jrs',\n",
    "    'silva,pit': 'tanno-silva,pi',\n",
    "    'tannosilva,pi': 'tanno-silva,pi',\n",
    "    'mendonca,r': 'mendonca,rc',\n",
    "    'mendonca,rr': 'mendonca,rc',\n",
    "    'schumke,j': 'schunke,j',\n",
    "    'abe,lb': 'abe,lm',\n",
    "    'stieber,m': 'stieber,mt',\n",
    "    'sieber,m': 'stieber,mt',\n",
    "    'chagasesilva,fc': 'chagas-e-silva,fc', \n",
    "    'chagasesilva,f': 'chagas-e-silva,fc',\n",
    "    'silva,fc': 'chagas-e-silva,fc',\n",
    "    'prance,g': 'prance,gt',\n",
    "    'melo,trb': 'mello,trb', \n",
    "    'sena,pac': 'senna,pac',\n",
    "    'pereira,l': 'pereira,la',\n",
    "    'caneiro,j': 'carneiro,j',\n",
    "    'munhoz,ca': 'munhoz,cbr',\n",
    "    'jimenez': 'jimenez,ja',\n",
    "    'castellanos': 'castellanos,a',\n",
    "    'cristobal,l': 'cristobal,cl', \n",
    "    'sousa,ng': 'souza,ng',\n",
    "    'westra,lyt': 'westra,lyth',\n",
    "    'luna,ta': 'luna,ti',\n",
    "    'pilger': 'pilges',\n",
    "    'silva,mi': 'silva,mib',\n",
    "    'mitzi': 'mitzi,g',\n",
    "    'santos,rr': 'santos,rrb',\n",
    "    'morales,r': 'morales,rav',\n",
    "    'siva,ja': 'silva,ja', \n",
    "    'vitti,f': 'vitti,fx',\n",
    "    'oliveira,fac': 'oliveira,fcao',\n",
    "    'landrum,s': 'landrum,ss',\n",
    "    'rodrigues,wa': 'rodrigues,wm',\n",
    "    'constable,ef': 'constable,ej',\n",
    "    'armando,m': 'armando,ms',\n",
    "    'falcao,ji': 'falcao,jia',\n",
    "    'maroccolo,jf': 'marocollo,jf',\n",
    "    'maxwell,h': 'maxwell,hh',\n",
    "    'zaruchi': 'zarucchi,j', \n",
    "    'santos,mcv': 'vilela-santos,mc', \n",
    "    'vilelasantos,mc': 'vilela-santos,mc',\n",
    "    'silva,eb': 'silva,ebm',\n",
    "    'jumbo,s': 'jimbo,s', \n",
    "    'medeiros,l': 'medeiros,lb',\n",
    "    'johnson,l': 'johnson,las',\n",
    "    'franciso,em': 'francisco,em', \n",
    "    'melo': 'melo,e',\n",
    "    'nogueira,lm': 'nogueira,lmg',\n",
    "    'aquino,f': 'aquino,fg',\n",
    "    'cairus,rjr': 'cairos,rjr',\n",
    "    'santos,aj': 'santos,ajv',\n",
    "    'cerqueira,ls': 'cerqueira,lsc',\n",
    "    'fonseca,fj': 'fonseca,js',\n",
    "    'fonseca,l': 'fonseca,lm',\n",
    "    'vilar,ts': 'villar,ts',\n",
    "    'colleta,gd': 'colletta,gd',\n",
    "    'barker,r': 'barker,rm',\n",
    "    'fragg,c': 'fagg,cw', \n",
    "    'fagg,c': 'fagg,cw',\n",
    "    'pennigton,td': 'pennington,td',\n",
    "    'rivera,v': 'rivera,vl',\n",
    "    'smya,s': 'sumya,s',\n",
    "    'versiane,af': 'versiane,afa',\n",
    "    'castelo,aj': 'castro,aj',\n",
    "    'richards': 'richardspwin',\n",
    "    'richards,m': 'richardspwin',\n",
    "    'richards,pw': 'richardspwin',\n",
    "    'pinheiro,s': 'pinheiros,s',\n",
    "    'pinheiro,em': 'pinheiros,em',\n",
    "    'lima,e': 'lima,es',\n",
    "    'hind,dj': 'hind,djn',\n",
    "    'hind,n': 'hind,djn', \n",
    "    'soares,g': 'soares,gf',\n",
    "    'ferreira,map': 'pereira,map',\n",
    "    'dobereiner': 'dobereiner,j',\n",
    "    'scariot,a': 'scariot,ao',\n",
    "    'monteiro,r': 'monteiro,rn',\n",
    "    'leal,cg': 'leal,g',\n",
    "    'garcia,mcm': 'garcia,mgm',\n",
    "    'lopes,wdp': 'lopes,wp',\n",
    "    'oliveira,m': 'oliveira,ms',\n",
    "    'vinha,s': 'vinha,sg',\n",
    "    'santos,g': 'santos,gb',\n",
    "    'filgueira,ts': 'filgueiras,ts',\n",
    "    'stutte,jg': 'stutts,jg',\n",
    "    'stutte,j': 'stutts,jg',\n",
    "    'stutts,j': 'stutts,jg',\n",
    "    'shutts,jg': 'stutts,jg',\n",
    "    'garcia,pb': 'garcia,pbc',\n",
    "    'joaovicente': 'vicente,jc',\n",
    "    'mariz,g': 'mariza,g',\n",
    "    'fierros,af': 'freire-fierros,a', \n",
    "    'freirefierros,a': 'freire-fierros,a',\n",
    "    'rodrigues,ce': 'rodrigues-junior,ce', \n",
    "    'rodriguesjunior,ce': 'rodrigues-junior,ce',\n",
    "    'kirkbridejunior,j': 'kirkbride-junior,jh', \n",
    "    'kirkbridejunior,jh': 'kirkbride-junior,jh',\n",
    "    'kirkbride,jh': 'kirkbride-junior,jh',\n",
    "    'souza,rs': 'sousa,rs',\n",
    "    'belizario,m': 'belisario,m',\n",
    "    'gibles,p': 'gibbs,pe', \n",
    "    'black': 'black,ga',\n",
    "    'black,g': 'black,ga',\n",
    "    'black,ca': 'black,ga',\n",
    "    'szechy,mtm': 'sechy,mts', \n",
    "    'ladeira,j': 'ladeira,jl',\n",
    "    'kauseilmari':'kause,i', \n",
    "    'rodrigues,w': 'rodrigues,wm',\n",
    "    'marines,g': 'marinis,g',\n",
    "    'maranis,g': 'marinis,g',\n",
    "    'bromley,g': 'bromley,gl',\n",
    "    'barbosa,ea': 'barbosa,e', \n",
    "    'leitaofilho,h':'leitao-filho,h',\n",
    "    'leitao,hf': 'leitao-filho,h',\n",
    "    'leitaofilho,hf':'leitao-filho,h',\n",
    "    'nilsson,s': 'nilson,s', \n",
    "    'mirizawa,m': 'kirizawa,m',\n",
    "    'gomes,v': 'klein,vlg',\n",
    "    'leite,jra': 'leite,jrs',\n",
    "    'philcox': 'philcox,d',\n",
    "    'dutil,jh': 'dutilh,jha',\n",
    "    'arraes,mgm': 'arrais,mgm',\n",
    "    'soderstrom': 'soderstrom,tr',\n",
    "    'raulino,t': 'raulino,taf',\n",
    "    'cisnero,la': 'cisneros,la',\n",
    "    'santos,f': 'santos,gf',\n",
    "    'santos,fm': 'santos,fam',\n",
    "    'santos,ffm': 'santos,fam',\n",
    "    'wasshusen,dc': 'wasshausen,dc', \n",
    "    'fereira,a': 'ferreira,a',\n",
    "    'freitas,g': 'freitas,gs',\n",
    "    'oliveira,fca': 'oliveira,fcao',\n",
    "    'cervi,ac': 'cervi,ca',\n",
    "    'wilsonbrowne,g': 'browne,gw', \n",
    "    'ribeiro,mm': 'ribeiro,mmv',\n",
    "    'boom,b': 'boom,bm',\n",
    "    'amorim,pr': 'amorim,prf',\n",
    "    'amorim,p': 'amorim,prf',\n",
    "    'lannasobrinho,jp': 'lana-sobrinho,jp',\n",
    "    'lanasobrinho,jp': 'lana-sobrinho,jp',\n",
    "    'sobrinho,jpl': 'lana-sobrinho,jp',\n",
    "    'pires,jm': 'pires,jn',\n",
    "    'pires,jf': 'pires,jn',\n",
    "    'bomley,gl': 'bromley,gl',\n",
    "    'dusi,rl': 'dusi,rlm',\n",
    "    'assuncao,pacl': 'assuncao,pac',\n",
    "    'assunsao,paci': 'assuncao,pac',\n",
    "    'assuncao,pacs': 'assuncao,pac',\n",
    "    'assuncao,pa': 'assuncao,pac',\n",
    "    'hathome,w': 'hawthorne,w',\n",
    "    'sendullsky,t': 'sendulsky,t',\n",
    "    'caixetadedeus,w': 'caixeta,w',\n",
    "    'alavarenga,d': 'alvarenga,d',\n",
    "    'fontella,pj': 'fontella,jp',\n",
    "    'andrade,p': 'andrade,pm',\n",
    "    'rizzini': 'rizzini,ct',\n",
    "    'barboza,ma': 'barbosa,ma'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rebuilding the network model with remapped names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the remapping dictionary above to remap our NamesMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.remap(remap,fromScratch=True)\n",
    "ni = nc.getNamesIndexes(occs, 'recordedBy_atomized',namesMap=nm.getMap())\n",
    "G = CoworkingNetwork( occs['recordedBy_atomized'], weighted=True, namesMap=nm )\n",
    "G.remove_node('etal')\n",
    "nx.set_node_attributes(G, 'n_records', dict( (n, len(ni[n])) for n in G.nodes() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a report of inconsistencies in the names map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nm.reportRemappingInconsistencies(returnFormatted=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names map would report an inconsistency if, for example, a name appears both as a key and a value in the remapping dict. For example, let's include `'abe,ln'`, which is already a value in the dict, as a key to `'abe,lb'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.remap({'abe,lm':'abe,lb'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now ask for a new report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These names appear both as keys and values:\n",
      "===========================================\n",
      "  abe,lb\n",
      "  abe,lm\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nm.reportRemappingInconsistencies(returnFormatted=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be fixed by removing `'abe,lm'` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abe,lb'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.remove_fromRemap('abe,lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nm.reportRemappingInconsistencies(returnFormatted=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `'abe,lb'` is still a key to `'abe,lm'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abe,lm'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm._remappingDict['abe,lb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can finally save the names map in a *.json* file for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm.write_toJson('namesMap.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the map is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm2 = read_namesMap('./namesMap.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names map just read\n",
      "===================\n",
      ".: \n",
      "1980 Sino-Amer Exped.: sinoamerexped\n",
      "?: \n",
      "A.J.N.V.: ajnv\n",
      "A.M.: am\n",
      "Abbas, B: abbas,b\n",
      "Abdala, GC: abdala,gc\n",
      "Abdo, MSA: abdo,msa\n",
      "Abdon: abdon\n",
      "Abe, LB: abe,lm\n",
      "Abe, LM: abe,lm\n",
      "Abrahim, MA: abrahim,ma\n",
      "Abreu, CG: abreu,cg\n",
      "Abreu, GX: abreu,gx\n",
      "Abreu, I: abreu,i\n",
      "Abreu, LC: abreu,lcr\n",
      "Abreu, LCR: abreu,lcr\n",
      "Abreu, M: abreu,mc\n",
      "Abreu, MC: abreu,mc\n",
      "Abreu, MS: abreu,mc\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Names map just read\\n===================\")\n",
    "print('\\n'.join( [k+\": \"+v for k,v in nm2.getMap().items()][:20] ))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note, though, that the normalization function cannot be stored in the *.json* file, and therefore it must be redefined in the names map copy we've just read. New names cannot be inserted in the map before defining the normalization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nm2._normalizationFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a normalization function must be defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7dff2c4f0165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pedro C. de Siracusa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-5410cbbd712e>\u001b[0m in \u001b[0;36minsertNames\u001b[0;34m(self, names, normalizationFunction, rebuild)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalizationFunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalizationFunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a normalization function must be defined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mnormalizationFunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalizationFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalizationFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a normalization function must be defined"
     ]
    }
   ],
   "source": [
    "nm2.insertNames(['Pedro C. de Siracusa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm2.setNormalizationFunc( nc.normalize )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm2.insertNames(['Pedro C. de Siracusa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the normalization function I could insert a new collector name. One alternative for also storing the normalization function of the names map would be to serialize it using the pickle library. The advantage of using *.json*, however, concerns readability of the map. One could even edit the map in a simple text editor and reload it in a python session. And that's it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm namesMap.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
